{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prad69/LLM/blob/main/RAG_LLM_Nvidia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iYBSNHlX5WP"
      },
      "source": [
        "\n",
        "# AI-Assisted Learning for NVIDIA SDKs and Toolkits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBFJU1V9YAzs"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYLceP4oklf8"
      },
      "outputs": [],
      "source": [
        "#@title Install Modules\n",
        "\n",
        "!pip install accelerate transformers tokenizers\n",
        "!pip install bitsandbytes einops\n",
        "!pip install xformers\n",
        "!pip install langchain\n",
        "!pip install faiss-gpu\n",
        "!pip install sentence_transformers\n",
        "!pip install -q langchain-openai langchain playwright beautifulsoup4\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VSVOHB-sYxG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d9e1ccf2-6eeb-48ad-8151-b4c85cce132b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Imports\n",
        "from torch import cuda, bfloat16\n",
        "import pickle\n",
        "import transformers\n",
        "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pickle\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from langchain.vectorstores import Chroma\n",
        "import langchain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import time\n",
        "import os\n",
        "langchain.debug = False\n",
        "\n",
        "#Settings for wrap text output for colab\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def my_css():\n",
        "   display(HTML(\"\"\"<style>table.dataframe td{white-space: nowrap;}</style>\"\"\"))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', my_css)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "E1-b-QHDGmUV",
        "outputId": "0fdf3973-aaed-45a8-c841-a63470106904"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Mount gdrive to save/load scraped data and Vector DB\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Functions"
      ],
      "metadata": {
        "id": "Aib20YTdRF-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Function - Load Model from Huggingface\n",
        "Helper function to download pretrained model from HuggingFace and save to disk.\n",
        "For subsequent runs, the saved model is loaded from disk.\n",
        "Note- Uncomment code to download and save the model to disk as needed"
      ],
      "metadata": {
        "id": "bEVZ1AC-Kv3t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gk2geMG5lhr-",
        "outputId": "0552e964-a7ab-4394-f6ee-9a7fdd106ca8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "def loadModel(model_id, hf_auth_token ,model_save_path):\n",
        "\n",
        "  model_config = transformers.AutoConfig.from_pretrained(\n",
        "      model_id,\n",
        "      use_auth_token=hf_auth_token\n",
        "  )\n",
        "\n",
        "  bnb_config = transformers.BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_quant_type='nf4',\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_compute_dtype=bfloat16\n",
        "  )\n",
        "\n",
        "#uncomment the below code to download the model from HF for the first time and save to disk.\n",
        "\n",
        "  # model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "  #     model_id,\n",
        "  #     trust_remote_code=True,\n",
        "  #     config=model_config,\n",
        "  #     quantization_config=bnb_config,\n",
        "  #     device_map='auto',\n",
        "  #     use_auth_token=hf_auth_token\n",
        "  # )\n",
        "\n",
        "  #model.save_pretrained(save_path)   # Save the model to the specified path\n",
        "  #print(f\"Model saved to {save_path}\")\n",
        "\n",
        "\n",
        "  # Load the model from the specified path\n",
        "  model = transformers.AutoModelForCausalLM.from_pretrained(model_save_path)\n",
        "\n",
        "  # enable evaluation mode to allow model inference\n",
        "  model.eval()\n",
        "\n",
        "  print(f\"Model loaded on {device}\")\n",
        "  return model\n",
        "\n",
        "#Define stopping criteria for the model\n",
        "def getStoppingCriteria(tokenizer):\n",
        "  stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "  stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "  stop_token_ids\n",
        "  stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "  stop_token_ids\n",
        "\n",
        "  # define custom stopping criteria object\n",
        "  class StopOnTokens(StoppingCriteria):\n",
        "      def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "          for stop_ids in stop_token_ids:\n",
        "              if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                  return True\n",
        "          return False\n",
        "\n",
        "  stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "  return stopping_criteria"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions - VectorDB\n",
        "Helper functions to split the scraped documents and save the embeddings to a Vector database.(ChromaDB or FAISS)"
      ],
      "metadata": {
        "id": "5cUoQRC7O862"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def saveVectorstoFAISS(docs ,embeddings,  save_path , text_splitter):\n",
        "  print(\"saving vectors to FAISS\")\n",
        "  all_splits = text_splitter.split_documents(docs)\n",
        "  vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
        "  vectorstore.save_local(save_path)\n",
        "  print(f\"Vectors saved to {save_path}\")\n",
        "\n",
        "\n",
        "def saveVectorstoChroma(docs ,embeddings,  save_path , text_splitter):\n",
        "  print(\"saving vectors to ChromaDB\")\n",
        "  all_splits = text_splitter.split_documents(docs)\n",
        "  print(len(all_splits))\n",
        "  Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=save_path)\n",
        "  print(f\"Vectors saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "00XsTMZ0O743",
        "outputId": "622a3741-6bee-4488-b5a1-e34c30d6c0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helper function to save /load scraped documents to File"
      ],
      "metadata": {
        "id": "e4nc87koL2ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def scrapeData(urls ,max_depth,file_path) :\n",
        "\n",
        "  for url in urls:\n",
        "    print(\"Scraping url ---->\", url)\n",
        "    loader = RecursiveUrlLoader(\n",
        "        url=url, max_depth=max_depth, extractor=lambda x: Soup(x, \"html.parser\").text\n",
        "    )\n",
        "    docs =loader.load()\n",
        "    print(f\"Loaded {len(docs)} documents from {url}\")\n",
        "    saveNewData(docs, file_path)\n",
        "\n",
        "def saveNewData(new_data, file_path):\n",
        "  print(f\"Saving {len(new_data)} documents to {file_path}\")\n",
        "\n",
        "  # Check if the pickle file exists\n",
        "  if not os.path.exists(file_path):\n",
        "    with open(file_path, \"wb\") as f:\n",
        "     print(\"Creating file as it doesnt exist\",file_path)\n",
        "     pickle.dump(new_data, f)\n",
        "  else :\n",
        "\n",
        "    # Load the existing pickle file\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        existing_data = pickle.load(f)\n",
        "\n",
        "    # Combine the existing data with the new data\n",
        "    new_data = existing_data + new_data\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        pickle.dump(new_data, f)\n",
        "\n",
        "\n",
        "def saveScrapedDataToFile(data, file_path):\n",
        "  with open(file_path, 'wb') as f:\n",
        "    pickle.dump(data, f)\n",
        "\n",
        "def loadScrapedDatafromFile(file_path):\n",
        "  with open(file_path, 'rb') as f:\n",
        "    docs = pickle.load(f)\n",
        "  return docs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dbed2cdPL1Dj",
        "outputId": "e81231dd-fc59-45ad-abdb-d5f51828fb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helper function to Query the model and print Response and Execution time"
      ],
      "metadata": {
        "id": "QO5-dUodNKsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def queryModel(chain,query,chat_history):\n",
        "  start_time = time.perf_counter ()\n",
        "\n",
        "  result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "  print(\"Question--->\",result['question'])\n",
        "  print(\"Answer-->\",result['answer'])\n",
        "  print(\"Source Docs-->\",result['source_documents'])\n",
        "  chat_history.append((query, result['answer']))\n",
        "  print(\"Response time--- %s seconds ---\" % (time.perf_counter () - start_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AHHQzSdlNI9Q",
        "outputId": "906e1277-1b45-42c7-c809-3218a8042900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Hepler Functions"
      ],
      "metadata": {
        "id": "LHYdcU0rQ_TA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to test the model.\n",
        "Here a Custom prompt is used to model the responses. A QA chain is created and the vectorstore and model are passed to get the result."
      ],
      "metadata": {
        "id": "HYN9wMI1NTq_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0nnDnicJ-Zob",
        "outputId": "15287981-1641-4108-95fc-85fc97d70034"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def testNvidiaQAModel(model,vectorstore):\n",
        "\n",
        "  template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use five sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "  {context}\n",
        "  Question: {question}\n",
        "  Helpful Answer:\"\"\"\n",
        "\n",
        "\n",
        "  qa_prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "  # creating chain\n",
        "  chain = ConversationalRetrievalChain.from_llm(model, vectorstore.as_retriever(), return_source_documents=True\n",
        "                                                ,combine_docs_chain_kwargs={\"prompt\": qa_prompt})\n",
        "\n",
        "  chat_history = []\n",
        "\n",
        "  print(\"-------------------------------\")\n",
        "  query_1 = \"what is NVIDIA Nsight compute?\"\n",
        "  queryModel(chain,query_1,chat_history)\n",
        "\n",
        "  print(\"-------------------------------\")\n",
        "  query_2 = \"What is the NVIDIA CUDA Toolkit?\"\n",
        "  queryModel(chain,query_2,chat_history)\n",
        "\n",
        "  print(\"-------------------------------\")\n",
        "  query_3 = \"How can I install NVIDIA CUDA Toolkit on Windows?\"\n",
        "  queryModel(chain,query_3,chat_history)\n",
        "\n",
        "\n",
        "  print(\"-------------------------------\")\n",
        "  query_4 = \"What is the difference between NVIDIA's BioMegatron and Megatron530B LLM?\"\n",
        "  queryModel(chain,query_4,chat_history)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to test the model with followup questions.\n",
        "Helper function to test the model. Here a Custom prompt is used to model the responses. A QA chain is created and the vectorstore and model are passed\n",
        "FOllowup questions are asked to test the model memory and relavence of the responses."
      ],
      "metadata": {
        "id": "ABrlTJMrL4Bp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TlyxHKpC-gxd",
        "outputId": "d7bcd3ee-2262-41e8-9fb4-5b6085e40f67"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "def testNvidiaQAModelMemory(llm , vectorstore):\n",
        "\n",
        "  template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "  Use five sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "  {context}\n",
        "  Question: {question}\n",
        "  Helpful Answer:\"\"\"\n",
        "\n",
        "\n",
        "  qa_prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "  # creating chain\n",
        "  qa_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True\n",
        "                                                ,combine_docs_chain_kwargs={\"prompt\": qa_prompt})\n",
        "\n",
        "  chat_history = []\n",
        "  query_1 = \"Shouldn’t gpu__cycles_active >= sm__cycles_active.max since SM active means the GPU is active? Or is my understanding incorrect?\"\n",
        "  queryModel(qa_chain,query_1,chat_history)\n",
        "\n",
        "  print(\"-------------------------------\")\n",
        "  followup_query_1 = \"Tell me more about this\"\n",
        "  queryModel(qa_chain,followup_query_1,chat_history)\n",
        "\n",
        "\n",
        "  print(\"-------------------------------\")\n",
        "  followup_query_2 = \"What are the advantages of this?\"\n",
        "  queryModel(qa_chain,followup_query_2,chat_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Execution"
      ],
      "metadata": {
        "id": "mg3B1t-5QunD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape data\n",
        "Scrape a list of URLs and extract text from the Web documents .\n",
        "The scraped documents are saved to file and then subsequently loaded for converting the data to vectors.\n"
      ],
      "metadata": {
        "id": "IYMwac7_MnWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9iwOv0EvykJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "scraped_data_file_path = '/content/drive/MyDrive/ColabNotebooks/datasets/NvidiaData/nvidia_scraped_docs.pkl'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "#Scrape Data Settings\n",
        "max_depth_to_scrape = 3 # set to 2 for faster scraping\n",
        "urls_to_scrape = [ \"https://docs.nvidia.com/\",\n",
        "                  \"https://forums.developer.nvidia.com/c/developer-tools/106\",\n",
        "                  \"https://developer.nvidia.com/blog\",\n",
        "                  \"https://medium.com/search?q=nvidia+sdk\"]\n",
        "\n",
        "#Note- Scrape Data only during the first run. All scraped data is stored to disk as it is a time consuming process\n",
        "#scrapeData(urls_to_scrape, max_depth_to_scrape , scraped_data_file_path)\n",
        "\n",
        "#load scraped data from disk\n",
        "docs = loadScrapedDatafromFile(scraped_data_file_path)  #the data file is shared in the zip file submitted\n",
        "print(f\"Loaded {len(docs)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save/Load - Vector DB"
      ],
      "metadata": {
        "id": "AUcU2en9NORS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhdF-DCEGy6t"
      },
      "outputs": [],
      "source": [
        "vectorDB_file_path = '/content/drive/MyDrive/ColabNotebooks/datasets/NvidiaData/FAISS_index'\n",
        "\n",
        "#Embedding model\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "#Text splitter settings .These hyperparameters can impact the model /\n",
        "chunk_size=1000\n",
        "chunk_overlap=20\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size= chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "\n",
        "#save embeddings to the FAISS Vector data store\n",
        "#saveVectorstoFAISS(docs, embeddings, vectorDB_file_path, text_splitter) ##uncomment if vectorDB needs to be created for the first time ,else load from the existing Vector DB\n",
        "#load saved data from VectorDB from file\n",
        "vectorstore = FAISS.load_local(vectorDB_file_path,embeddings)\n",
        "\n",
        "\n",
        "#save embeddings to the Chroma Vector data store\n",
        "# saveVectorstoChroma(docs, embeddings, vectorDB_file_path, text_splitter)\n",
        "# #load previously saved Docs from Chroma DB\n",
        "# vectorstore = Chroma(persist_directory=vectorDB_file_path, embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Language Model\n"
      ],
      "metadata": {
        "id": "Atid9jNoN-Na"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "OzMjNdUzkqrq",
        "outputId": "b5db732c-3b2d-45e8-e405-b13bb1fd031a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Model used for the project\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "#Huggingface token\n",
        "hf_auth_token = 'xxxxxxxxxx'\n",
        "# Define the path where you want to save the model\n",
        "model_save_path = \"/content/drive/MyDrive/ColabNotebooks/Models/Llama-2-7b-chat-hf.pt\"\n",
        "\n",
        "pretrained_model = loadModel(model_id, hf_auth_token,model_save_path)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "      model_id,\n",
        "      use_auth_token=hf_auth_token\n",
        "  )\n",
        "\n",
        "\n",
        "qa_pipeline = transformers.pipeline(\n",
        "      model=pretrained_model,\n",
        "      tokenizer=tokenizer,\n",
        "      return_full_text=True,\n",
        "      #task='document-question-answering',\n",
        "      task='text-generation',\n",
        "      stopping_criteria=getStoppingCriteria(tokenizer),\n",
        "      temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "      max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "      repetition_penalty=1.1  # without this output begins repeating\n",
        "  )\n",
        "\n",
        "nvidia_qa_model_pipeline = HuggingFacePipeline(pipeline=qa_pipeline)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Nvidia QA Model"
      ],
      "metadata": {
        "id": "QSRV-drUON1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testNvidiaQAModel(nvidia_qa_model_pipeline , vectorstore)\n",
        "\n",
        "#test with Initial queries and followup queries\n",
        "testNvidiaQAModelMemory(nvidia_qa_model_pipeline, vectorstore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "_pdnXJUaA3ft",
        "outputId": "e86513f4-4ffc-4300-de7a-d8d864d4bed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>table.dataframe td{white-space: nowrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question---> what is NVIDIA Nsight compute?\n",
            "Answer-->  NVIDIA Nsight Compute is a system-wide performance analysis tool designed to visualize an application's algorithms. It provides various features such as kernel profiling, customization options, and training resources. Thanks for asking!\n",
            "Source Docs--> [Document(page_content='Nsight Compute Documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNVIDIANsight Compute Documentation\\n\\nSearch In:\\nEntire Site\\nJust This Document\\nclear search\\nsearch\\n\\n\\n\\nNsight Compute\\n\\nRelease Notes\\nKernel Profiling Guide\\nNsight Compute\\nNsight Compute CLI\\n\\nDeveloper Interfaces\\n\\nCustomization Guide\\nNvRules API\\n\\nTraining\\n\\nTraining\\n\\nRelease Information\\n\\nArchives\\n\\nCopyright And Licenses\\n\\nCopyright and Licenses\\n\\n\\n\\n\\nSearch Results\\n\\n\\n\\n\\nNsight Compute', metadata={'source': 'https://docs.nvidia.com/nsight-compute/', 'title': 'Nsight Compute Documentation', 'language': 'en'}), Document(page_content='Release Notes\\n\\r\\n                     Release notes, including new features and important bug fixes.\\r\\n                     Supported platforms and GPUs.\\r\\n                     List of known issues for the current release.\\r\\n                     \\r\\n                  \\nKernel Profiling Guide\\n\\r\\n                     Kernel Profiling Guide with metric types and meaning, data collection modes and FAQ for common problems.\\r\\n                     \\r\\n                  \\nNsight Compute\\n\\r\\n                     NVIDIA Nsight Compute User Interface (UI) manual.\\r\\n                     Information on all views, controls and workflows within the tool UI.\\r\\n                     Transitions guide for Visual Profiler.\\r\\n                     \\r\\n                  \\nNsight Compute CLI\\n\\r\\n                     NVIDIA Nsight Compute Command Line Interface (CLI) manual.\\r\\n                     Information on workflows and options for the command line, including multi-process profiling and NVTX filtering.', metadata={'source': 'https://docs.nvidia.com/nsight-compute/', 'title': 'Nsight Compute Documentation', 'language': 'en'}), Document(page_content='Developer Interfaces\\n\\n\\nCustomization Guide\\n\\r\\n                     User manual on customizing NVIDIA Nsight Compute tools or integrating them with custom workflows.\\r\\n                     Information on writing section files, rules for automatic result analysis and scripting access to report files.\\r\\n                     \\r\\n                  \\nNvRules API\\n\\r\\n                     The NvRules API reference for writing python-based analysis rules or scripted report access.\\r\\n                     \\r\\n                  \\n\\nTraining\\n\\n\\nTraining\\n\\r\\n                     NVIDIA Nsight Compute Training resources.\\r\\n                     \\r\\n                  \\n\\nRelease Information\\n\\n\\nArchives\\nFind documentation for previous versions of NVIDIA Nsight Compute.\\n\\nCopyright And Licenses\\n\\n\\nCopyright and Licenses\\n\\r\\n                     Information on the NVIDIA Software License Agreement as well as\\r\\n                     third party software and tools used by Nsight Compute.', metadata={'source': 'https://docs.nvidia.com/nsight-compute/', 'title': 'Nsight Compute Documentation', 'language': 'en'}), Document(page_content='Nsight Systems\\nA system-wide performance analysis tool designed to visualize an\\r\\n                                    applicationâ\\x80\\x99s algorithms.\\nBrowse\\n\\n\\nNsight Graphics\\nA standalone application for the debugging, profiling, and analysis\\r\\n                                    of graphics applications.\\nBrowse\\n\\n\\n\\n\\nDriveWorks 2.2 SDK\\nA comprehensive library of modules, developer tools, and reference\\r\\n                                    applications that take advantage of the computing power of the NVIDIA DRIVEâ\\x84¢\\r\\n                                    platform.\\n\\nBrowse\\n\\n\\n\\nCUDA Toolkit\\nA parallel computing platform and programming model developed by\\r\\n                                    NVIDIA for general computing on graphical processing units (GPUs).\\n\\nBrowse', metadata={'source': 'https://docs.nvidia.com/drive/index.html', 'title': 'Autonomous Vehicle Development Platforms | NVIDIA Docs', 'description': 'Learn how to develop for NVIDIA DRIVE®, a scalable computing platform that enables automakers and Tier-1 suppliers to accelerate production of autonomous vehicles.', 'language': 'en'})]\n",
            "Response time--- 9.48895741400338 seconds ---\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question---> What is the NVIDIA CUDA Toolkit?\n",
            "Answer-->   The NVIDIA CUDA Toolkit is a comprehensive development environment for C and C++ developers building GPU-accelerated applications. It provides a range of tools and libraries to help developers develop, optimize, and deploy their applications on various hardware platforms, including embedded systems, desktop workstations, enterprise data centers, cloud-based platforms, and HPC supercomputers. Thanks for asking!\n",
            "Source Docs--> [Document(page_content='The NVIDIA® CUDA® Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers. The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application.Using built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers can develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation Center\\n\\n\\nNVIDIA CUDA Toolkit Documentation\\n\\n\\n        These documents provide information regarding the current NVIDIA CUDA release.\\n    \\nFebruary 02, 2023 01:00 PM\\n\\n\\n\\n\\nCUDA Math Libraries\\n\\n\\n\\n\\n\\n\\nNVIDIA cuBLAS Library', metadata={'source': 'https://docs.nvidia.com/cuda/doc/index.html', 'title': 'NVIDIA CUDA - NVIDIA Docs', 'description': 'The NVIDIA® CUDA® Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.', 'language': 'en'}), Document(page_content='CUDA Toolkit Documentation 12.3 Update 2ï\\x83\\x81\\nDevelop, Optimize and Deploy GPU-Accelerated Apps\\nThe NVIDIAÂ® CUDAÂ® Toolkit provides a development environment for creating high performance GPU-accelerated\\r\\napplications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated\\r\\nembedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.\\r\\nThe toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime\\r\\nlibrary to deploy your application.\\nUsing built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers\\r\\ncan develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs.\\n\\n\\nRelease NotesThe Release Notes for the CUDA Toolkit.\\n\\nCUDA Features ArchiveThe list of CUDA features by release.', metadata={'source': 'https://docs.nvidia.com/cuda/index.html', 'title': 'CUDA Toolkit Documentation 12.3 Update 2', 'language': 'en'}), Document(page_content='Figure 3. CUDA\\n\\n                        The CUDA Toolkit is generally optional when GPU nodes are only used to run applications \\n                        (as opposed to develop applications) as the CUDA application typically packages (by statically or \\n                        dynamically linking against) the CUDA runtime and libraries needed. \\n                        \\n                     \\n\\nTypical Workflow', metadata={'source': 'https://docs.nvidia.com/datacenter/tesla/drivers/index.html', 'title': 'NVIDIA Datacenter Drivers :: NVIDIA Data Center GPU Driver Documentation', 'description': 'Documentation for NVIDIAÂ® Datacenter Drivers.', 'language': 'en-us'}), Document(page_content='The NVIDIAÂ® CUDAÂ® Toolkit enables developers to build NVIDIA GPU\\n                        accelerated compute applications for desktop computers, enterprise, and data centers to\\n                        hyperscalers. It consists of the CUDA compiler toolchain including the CUDA runtime\\n                        (cudart) and various CUDA libraries and tools. To build an application, a developer has\\n                        to install only the CUDA Toolkit and necessary libraries required for linking.\\n                     \\nIn order to run a CUDA application, the system should have a CUDA enabled GPU and an\\n                        NVIDIA display driver that is compatible with the CUDA Toolkit that was used to build\\n                        the application itself. If the application relies on dynamic linking for libraries, then\\n                        the system should have the right version of such libraries as well.\\n                     \\n\\nFigure 1. Components of CUDA', metadata={'source': 'https://docs.nvidia.com/deploy/cuda-compatibility/index.html', 'title': 'CUDA Compatibility  :: NVIDIA Data Center GPU Driver Documentation', 'description': 'CUDA Compatibility document describes the use of new CUDA toolkit components on systems with older base installations.', 'language': 'en-us'})]\n",
            "Response time--- 15.317032473998552 seconds ---\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question---> How can I install NVIDIA CUDA Toolkit on Windows?\n",
            "Answer-->   Thanks for asking! To install NVIDIA CUDA Toolkit on Windows, you can follow these steps:\n",
            "\n",
            "1. Download the NVIDIA driver from the official website and install it on your computer.\n",
            "2. Install the CUDA Toolkit by running the installer and following the on-screen instructions.\n",
            "3. Once the installation is complete, open a command prompt and type \"cuda-toolkit-11-0\" to verify the installation.\n",
            "\n",
            "If you have any further questions or concerns, feel free to ask!\n",
            "Source Docs--> [Document(page_content='CUDA Toolkit\\n\\n\\n                           After installing the NVIDIA driver, Fabric Manager and NSCQ, you can proceed to \\n                           install the CUDA Toolkit on the system to build CUDA applications. Note that if you \\n                           are deploying CUDA applications only, then the CUDA Toolkit is not necessary as \\n                           the CUDA application should include the dependencies it needs. \\n                           \\n                        \\n\\n                           To install CUDA Toolkit, letâ\\x80\\x99s use the cuda-toolkit-11-0 meta-package. For other meta-packages, \\n                           review this table in the documentation. This meta-package installs only the CUDA Toolkit (and does not install the NVIDIA driver).\\n                           \\n                        \\n\\n                           Check the meta-packages available using the following command:', metadata={'source': 'https://docs.nvidia.com/datacenter/tesla/hgx-software-guide/index.html', 'title': 'NVIDIA HGX A100 Software User Guide :: NVIDIA Tesla Documentation', 'description': 'User guide for setting up software on NVIDIAÂ® HGX A100.', 'language': 'en-us'}), Document(page_content='The NVIDIAÂ® CUDAÂ® Toolkit enables developers to build NVIDIA GPU\\n                        accelerated compute applications for desktop computers, enterprise, and data centers to\\n                        hyperscalers. It consists of the CUDA compiler toolchain including the CUDA runtime\\n                        (cudart) and various CUDA libraries and tools. To build an application, a developer has\\n                        to install only the CUDA Toolkit and necessary libraries required for linking.\\n                     \\nIn order to run a CUDA application, the system should have a CUDA enabled GPU and an\\n                        NVIDIA display driver that is compatible with the CUDA Toolkit that was used to build\\n                        the application itself. If the application relies on dynamic linking for libraries, then\\n                        the system should have the right version of such libraries as well.\\n                     \\n\\nFigure 1. Components of CUDA', metadata={'source': 'https://docs.nvidia.com/deploy/cuda-compatibility/index.html', 'title': 'CUDA Compatibility  :: NVIDIA Data Center GPU Driver Documentation', 'description': 'CUDA Compatibility document describes the use of new CUDA toolkit components on systems with older base installations.', 'language': 'en-us'}), Document(page_content='Under Operating System, click Linux.\\nUnder Distribution, click Ubuntu.\\nUnder Installer Type, click deb (network).\\nTo add the CUDA repository to the system, follow the steps under Installation Instructions.\\n \\nUpdate the apt repository cache.\\n\\n\\n\\nCopy\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n            $ sudo apt-get update\\n        \\n\\n\\n\\nInstall the compatibility package. \\n\\n\\n\\nCopy\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n            $ sudo apt-get install -y cuda-compat-11-8\\n        \\n\\n\\n The commands in this step will install the compatibility package libraries in the /usr/local/cuda-11.8/compat folder.  \\nAppend this path to LD_LIBRARY_PATH when the SDK applications are run. \\n\\n\\n\\nCopy\\nCopied!\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n            # Add path to LD_LIBRARY_PATH', metadata={'source': 'https://docs.nvidia.com/deeplearning/maxine/audio-effects-sdk/index.html', 'title': 'Audio Effects SDK Programming Guide - NVIDIA Docs', 'description': 'The Audio Effects SDK Programming Guide provides audio effects for broadcast use cases with real-time audio processing.', 'language': 'en'}), Document(page_content='EULAThe CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.\\n\\n\\n\\n\\nInstallation Guidesï\\x83\\x81\\n\\nQuick Start GuideThis guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system.\\n\\nInstallation Guide WindowsThis guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems.\\n\\nInstallation Guide LinuxThis guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems.\\n\\n\\n\\n\\n\\nProgramming Guidesï\\x83\\x81', metadata={'source': 'https://docs.nvidia.com/cuda/index.html', 'title': 'CUDA Toolkit Documentation 12.3 Update 2', 'language': 'en'})]\n",
            "Response time--- 19.843239135996555 seconds ---\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question---> What is the difference between NVIDIA's BioMegatron and Megatron530B LLM?\n",
            "Answer-->  Thanks for asking! NVIDIA's BioMegatron and Megatron530B LLMs are both large language models, but they have some differences. BioMegatron is a variant of the Megatron model that uses a different training strategy, called \"biological\" training, which involves fine-tuning the model on a variety of biological datasets. This approach can result in better performance on certain tasks, such as text classification or sentiment analysis. On the other hand, Megatron530B is a more recent version of the Megatron model that has been further optimized for improved performance on a wide range of NLP tasks. So while BioMegatron may have some advantages in certain areas, Megatron530B is likely to be a more versatile and powerful option for many use cases.\n",
            "Source Docs--> [Document(page_content='All of these products (nvidia-smi, NVML, and the NVML language bindings) are updated with each new CUDA release and provide roughly the same functionality.\\nSee https://developer.nvidia.com/nvidia-management-library-nvml for additional information.', metadata={'source': 'https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html', 'title': '1. Preface â\\x80\\x94 CUDA C++ Best Practices Guide 12.3 documentation', 'description': 'The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs.', 'language': 'en'}), Document(page_content='2048w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nemotron-3-8b-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nemotron-3-8b-960x540.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nemotron-3-8b-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nemotron-3-8b-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nemotron-3-8b-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nemotron-3-8b-196x110.png 196w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/nemotron-3-8b-1024x576.png 1024w\" sizes=\"(max-width: 768px) 100vw, 768px\" title=\"nemotron-3-8b\" />Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning....', metadata={'source': 'https://developer.nvidia.com/blog/category/conversational-ai/feed/', 'title': 'Conversational AI – NVIDIA Technical Blog'}), Document(page_content='0\\n\\n\\n\\nAmanda Saunders\\n\\nUnlocking the Power of Enterprise-Ready LLMs with NVIDIA NeMo\\n\\nhttps://developer.nvidia.com/blog/?p=68768\\n2024-02-22T00:58:02Z\\n2023-08-08T18:35:11Z', metadata={'source': 'https://developer.nvidia.com/blog/category/conversational-ai/feed/', 'title': 'Conversational AI – NVIDIA Technical Blog'}), Document(page_content='NVIDIA GH200 SuperChip-based MGX servers are shipping now. These servers combine a Hopper GPU with a 72-core Grace CPU and memory on a module and are designed for accelerated applications that are tightly coupled with the underlying CPU and memory platform. This guide covers the unboxing and basic operating system installation, driver and runtime setup, and configuration of a typical platform.\\n    \\n\\n\\n\\n            Browse\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNVIDIA GH200 Grace Hopper Superchip Benchmark Step-by-Step Guide\\n\\n\\n        This application note provides NVIDIA GH200 benchmark data in comparison to the NVIDIA® DGX™ H100 platform.\\n    \\n\\n\\n\\n            Browse\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCorporate Info\\n\\n\\nNVIDIA.com Home\\n\\nAbout NVIDIA\\n\\n\\n\\n\\n\\n\\n\\n\\u200eNVIDIA Developer\\n\\n\\nDeveloper Home\\n\\nBlog\\n\\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\nContact Us\\n\\nDeveloper Program\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © 2024 NVIDIA CorporationLegal Information | Privacy | Cookie Policy | Contact', metadata={'source': 'https://docs.nvidia.com/grace/index.html', 'title': 'NVIDIA Grace - NVIDIA Docs', 'description': 'Grace is NVIDIA’s first datacenter CPU. Comprising 72 high-performance Arm v9 cores and featuring the NVIDIA-proprietary Scalable Coherency Fabric (SCF) network-on-chip for incredible core-to-core communication, memory bandwidth and GPU I/O capabilities, Grace provides a high-performance compute foundation in a low-power system-on-chip.', 'language': 'en'})]\n",
            "Response time--- 28.205009486999188 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1RuyUdrXTxPV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNP3MYqelZjUrFYVxqwusjo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
